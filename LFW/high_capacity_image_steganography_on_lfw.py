# -*- coding: utf-8 -*-
"""High-Capacity-Image-Steganography-on-LFW.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LMA21YHCXsCsBQ8bKoCrGJWxNExHG3Ss

# important tips 
*the structure of tensor for image is (Channel , Hieght , Width)
* in densenet for each layer the feature maps of all preceding layers and its own feature maps are used as input for subsequent layer so concatenation must be done in depth

# Code
"""

############ Necessary labraries#############
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F
from torch import nn
from torch.utils.data import DataLoader,Dataset
from torchvision.datasets import LFWPeople
from torchvision import transforms
from time import time
from skimage.data import astronaut,rocket
from skimage.metrics import structural_similarity
from torch.nn.modules.pooling import MaxPool2d
device = ('cuda' if torch.cuda.is_available() else 'cpu')
!pip install torchmetrics
from torchmetrics import StructuralSimilarityIndexMeasure

############ Required transformations on dataset #############

mytransforms = transforms.Compose([
                                   transforms.ToTensor(),
                                   transforms.Resize((128,128))])

############ Downloading Dataset #############
### it doesn't reduce your internt traffic ###

train_data = LFWPeople('./data',download=True,split='train',transform=mytransforms)
test_data = LFWPeople('./data',download=True,split='test',transform=mytransforms)

############ Providing Dataloaders for train and test data #############

train_loader = DataLoader(train_data, batch_size = 16, num_workers = 2)
test_loader = DataLoader(test_data, batch_size = 16, num_workers = 2)

############ Defining classes which are used in Network #############

############ DenseNet is composed of Single layers  #############

class SingleLayer(nn.Module):
    def __init__(self, nChannels, growthRate):
        super(SingleLayer, self).__init__()
        self.bn     = nn.BatchNorm2d(nChannels)
        self.conv   = nn.Conv2d(nChannels, growthRate, kernel_size=3,padding=1, bias=False)
        self.relu   = nn.ReLU()
        self.dropout = nn.Dropout2d(0.2)

    def forward(self, x):
        out = self.bn(x)
        out = self.conv(out)
        out = self.relu(out)
        out = self.dropout(out)
        out = torch.cat((x, out), 1)
        return out
####################################################################################################

############ DenseNet is made by choosing number of layers and growth rate which is a constant output of each layer  #############

class DenseNet(nn.Module):
  def __init__(self, nChannels, growthRate, nDenseBlocks):
    super(DenseNet,self).__init__()
    self.dense = self.make_dense(nChannels, growthRate, nDenseBlocks)
  
  def make_dense(self, nChannels, growthRate, nDenseBlocks):
    layers = []
    for i in range(int(nDenseBlocks)):
        layers.append(SingleLayer(nChannels, growthRate))
        nChannels += growthRate
    return nn.Sequential(*layers)  

  def forward(self,x):
    out = self.dense(x)
    return out


####################################################################################################

############ Transition down layer is used to reduce the number of feature maps  #############

class Transition_Down(nn.Module): 
  def __init__(self, input_channels,output_channels):
    super(Transition_Down, self).__init__()
    self.TD = nn.Sequential(nn.BatchNorm2d(input_channels),
                            nn.ReLU(),
                            nn.Conv2d(input_channels,output_channels,kernel_size=1,padding ='same'),
                            nn.MaxPool2d(kernel_size=2,stride=2))
    
  def forward(self,x):
    out = self.TD(x)
    return out  
    
####################################################################################################

############ Transition up layer is used to compensate the effect of transtion down layer which has reduce nemuber of feature maps  #############

class Transition_Up(nn.Module): 
  def __init__(self, input_channels,output_channels):
    super(Transition_Up, self).__init__()
    self.TU = nn.ConvTranspose2d(input_channels,output_channels, kernel_size=2, stride=2 , padding=0)
    
  def forward(self,x):
    out = self.TU(x)
    return out  
    
####################################################################################################

############ Decoder block is used in reveal network ro extract secret image #############
class Decoder_block(nn.Module): 
  def __init__(self, input_channels , output_channels):
    super(Decoder_block, self).__init__()
    self.DecoderBlock = nn.Sequential(nn.Conv2d(input_channels , output_channels , kernel_size=3 , padding =1 ,stride=1),
                            nn.BatchNorm2d(output_channels),
                            nn.ReLU())
    
  def forward(self,x):
    out = self.DecoderBlock(x)
    return out

############ Defining Hidden Network by using the classes which are defined previously #############

class Hidden_Network(nn.Module):
  def __init__(self):
    super(Hidden_Network,self).__init__()
    self.conv1 = nn.Conv2d( 6, 48, kernel_size = 3 , padding = 'same')
    self.DB1   = DenseNet (48,12,4)
    self.TD1   = Transition_Down(144,96)
    self.DB2   = DenseNet (96,12,4)
    self.TD2   = Transition_Down(240,144)
    self.DB3   = DenseNet (144,12,4)
    self.TD3   = Transition_Down(336,192)
    self.DB4   = DenseNet (192,12,4)
    self.TD4   = Transition_Down(432,240)
    self.DB5   = DenseNet (240,12,4)
    self.TD5   = Transition_Down(528,288)
    self.DB6   = DenseNet (288,12,4)
    self.TU1   = Transition_Up (336,336)
    self.DB7   = DenseNet (864,12,4)
    self.TU2   = Transition_Up (912,112)
    self.DB8   = DenseNet (544,12,4)
    self.TU3   = Transition_Up (592,92)
    self.DB9   = DenseNet (428,12,4)
    self.TU4   = Transition_Up (476,76)
    self.DB10  = DenseNet (316,12,4)
    self.TU5   = Transition_Up (364,64)
    self.DB11  = DenseNet (208,12,4)
    self.conv2 = nn.Conv2d( 256, 3, kernel_size = 3 , padding = 'same')
  def forward(self, input):
    y   = self.conv1(input)
    DB1 = self.DB1(y)
    C1  = torch.cat((y, DB1), 1)
    TD1 = self.TD1(C1)
    DB2 = self.DB2(TD1)
    C2  = torch.cat((DB2, TD1), 1)
    TD2 = self.TD2(C2)
    DB3 = self.DB3(TD2)
    C3  = torch.cat((DB3, TD2), 1)
    TD3 = self.TD3(C3)
    DB4 = self.DB4(TD3)
    C4  = torch.cat((DB4, TD3), 1)
    TD4 = self.TD4(C4)
    DB5 = self.DB5(TD4)
    C5  = torch.cat((DB5, TD4), 1)
    TD5 = self.TD5(C5)
    DB6 = self.DB6(TD5)
    TU1 = self.TU1(DB6)
    C6  = torch.cat((TU1, C5), 1)
    DB7 = self.DB7(C6)
    TU2 = self.TU2(DB7)
    C7  = torch.cat((TU2, C4), 1)
    DB8 = self.DB8(C7)
    TU3 = self.TU3(DB8)
    C8  = torch.cat((TU3, C3), 1)
    DB9 = self.DB9(C8)
    TU4 = self.TU4(DB9)
    C9  = torch.cat((TU4, C2), 1)
    DB10= self.DB10(C9)
    TU5 = self.TU5(DB10)
    C10 = torch.cat((TU5, C1), 1)
    DB11= self.DB11(C10)
    output = self.conv2(DB11)
    return output

########## Testing the Hidden Network ###########
a = torch.rand(1,6,256,256)
Hidden_Network().forward(a).shape

############ Defining Reveal Network by using the classes which are defined previously #############

class Reveal_Network(nn.Module):

  def __init__(self):
    super(Reveal_Network,self).__init__()
    self.Decoder1 = Decoder_block(3,64)
    self.Decoder2 = Decoder_block(64,128)
    self.Decoder3 = Decoder_block(128,256)
    self.Decoder4 = Decoder_block(256,128)
    self.Decoder5 = Decoder_block(128,64)
    self.conv1    = nn.Conv2d( 64 , 3 , kernel_size=3 , padding =1 ,stride=1)
    self.sigmo    = nn.Sigmoid()

  def forward(self, input):
    Decoder1  = self.Decoder1(input)
    Decoder2  = self.Decoder2(Decoder1)
    Decoder3  = self.Decoder3(Decoder2)
    Decoder4  = self.Decoder4(Decoder3)
    Decoder5  = self.Decoder5(Decoder4)
    output    = self.conv1(self.sigmo(Decoder5))
    
    return output

########## Testing the Reveal Network ###########
a = torch.rand(1,3,256,256)
Reveal_Network().forward(a).shape

########## Defining Loss Function classes for each network ###########
########## we used one of quatative evaluation methods (Mean Square Error) to for the loss function ###########
class Stegano_Network(nn.Module):

  def __init__(self):
    super(Stegano_Network,self).__init__()
    self.hidden=Hidden_Network()
    self.reveal=Reveal_Network()
  def forward(self, input):
    Stego = self.hidden(input)
    revealed_image = self.reveal(Stego)
    
    return Stego,revealed_image

########## Defining Loss Function classes for each network ###########
class EncoderLoss(nn.Module):
  def __init__(self):
      super(EncoderLoss,self).__init__()
        
  def forward(self,carrier_image,stego_image):    
      encoder_loss = (F.mse_loss(carrier_image,stego_image))
      return encoder_loss

class DecoderLoss(nn.Module):
  def __init__(self):
      super(DecoderLoss,self).__init__()
        
  def forward(self,revealed_image,secret_image):
      decoder_loss = (F.mse_loss(revealed_image,secret_image))
      return decoder_loss
class SteganoLoss(nn.Module):
  def __init__(self,decoder_weight):
      super(SteganoLoss,self).__init__()
      self.decoder_weight = decoder_weight
        
  def forward(self,carrier_image,stego_image,revealed_image,secret_image):
      stegano_loss = (F.mse_loss(carrier_image,stego_image)) + (self.decoder_weight*(F.mse_loss(revealed_image,secret_image)))
      return stegano_loss

device = ('cuda' if torch.cuda.is_available() else 'cpu')
mymodel= Stegano_Network().to(device)

'''
 DO NOT run this block 
'''
########## Traning loops ###########

epochs = 2    
'''
 The numbers of epochs is small because our dataser has 9525 images so in each epoch our network will be trained by 50000 images, 
 consequently our network will be saturated very soon so this number of epochs is enough 
'''
batch_size=16
beta = 0.75
criterion_encoder=EncoderLoss()
criterion_decoder=DecoderLoss()
criterion = SteganoLoss(beta)   # 0.75 is beta hyperparamete in the paper
opt = torch.optim.Adam(mymodel.parameters())

encoder_losses   = [] # hidden Losses in different epochs
decoder_losses   = [] # reveal Losses in different epochs
losses           = [] # whole network Losses in different epochs
val_losses       = [] # whole network validation losses in different epochs
# carrier=[]
# secret=[]
last_loss=1
for epoch in range(epochs):
  for batch_i, (inputs, _) in enumerate(train_loader):
    tic = time() # Start of epoch
    # Train
    mymodel = mymodel.train()
    batch_losses = []
    encoder_batch_losses = []
    decoder_batch_losses = []
    inputs = inputs.to(device) # Images
    size=inputs.size()
    carrier=torch.empty(int(batch_size/2),size[1],size[2],size[3])
    secret=torch.empty(int(batch_size/2),size[1],size[2],size[3])
    concatenated_input=torch.empty(int(batch_size/2),2*size[1],size[2],size[3])
    j=0
    z=0
    for i in range(len(inputs)):
      if i%2 == 0 : 
        carrier[j]=inputs[i]
        j=j+1
      else :
        secret[z]=inputs[i]
        concatenated_input[z] = torch.cat((carrier[j-1],secret[z]),0)
        z=z+1

    concatenated_input = concatenated_input.to(device)
    carrier=carrier.to(device)
    secret=secret.to(device)
    
    Stego_image,revealed_image = mymodel.forward(concatenated_input)
    encoder_loss=criterion_encoder(carrier,Stego_image)
    decoder_loss=criterion_decoder(revealed_image,secret)
    opt.zero_grad()
    loss = criterion.forward(Stego_image, carrier,revealed_image,secret)
    loss.backward()
    opt.step()
    batch_losses.append(loss.item())
    encoder_batch_losses.append(encoder_loss.item())
    decoder_batch_losses.append(decoder_loss.item())
    train_string1 = f"information of each batch : batch : {batch_i} // batch_loss : {loss.item(): 0.7f} // encoder_loss : {encoder_loss.item(): 0.7f} // decoder_loss : {decoder_loss.item(): 0.7f} "
    if ((loss.item()) < last_loss):
      torch.save(mymodel.state_dict(), '/content/drive/MyDrive/Saved_Stegano_model_onLFW_ORG.pt')
    if batch_i%27 == 0 :
      print(train_string1)    
    
  epoch_loss = np.mean(batch_losses)
  encoder_epoch_loss = np.mean(encoder_batch_losses)
  decoder_epoch_loss = np.mean(decoder_batch_losses)
  losses.append(epoch_loss)
  train_string2 = f"information of each epoch: Epoch : {epoch} // loss : {epoch_loss: 0.7f} // encoder_loss : {encoder_epoch_loss: 0.7f} // decoder_loss : {decoder_epoch_loss: 0.7f} "  
  print(train_string2)

'''
 here you can use my saved model that I have sent just upload it to your colab and then write its directory
 here -> (torch.load(directory))
'''
mysaved_model=Stegano_Network()
mysaved_model.load_state_dict(torch.load('/content/drive/MyDrive/Saved_Stegano_model_onLFW_ORG.pt',map_location=torch.device('cpu')))

############# Visual & Quantative evaluation ###############

############# here we use our test images to evaluate our network #############

MSE = DecoderLoss()

for batch_i, (inputs, _) in enumerate(test_loader):
  x=inputs
  break

c1 = torch.cat((x[2],x[7]),0)  # test images have been selected randomly
c2 = torch.cat((x[0],x[1]),0)

c_reshape1=torch.reshape(c1,(1,6,128,128))
c_reshape2=torch.reshape(c2,(1,6,128,128))
stego1,revealed1=mysaved_model.forward(c_reshape1)
stego2,revealed2=mysaved_model.forward(c_reshape2)
stego_x1=stego1.detach()
stego_x2=stego2.detach()
revealed_x1=revealed1.detach()
revealed_x2=revealed2.detach()
secret1 = torch.reshape(x[11],(1,3,128,128))
secret2 = torch.reshape(x[12],(1,3,128,128))
carrier1= torch.reshape(x[10],(1,3,128,128))
carrier2= torch.reshape(x[3],(1,3,128,128))

''' 
Quantative evaluation methods :
    MSE
    PSNR
    SSIM
'''
#### SSIM ###
ssim = StructuralSimilarityIndexMeasure()
ssim_r_s1= ssim(revealed1, secret1)
ssim_r_s2= ssim(revealed2, secret2)
ssim_c_s1= ssim(stego1, carrier1)
ssim_c_s2= ssim(stego2, carrier2)
#### MSE  ###
mse_r_S1 = MSE(revealed1, secret1)
mse_r_S2 = MSE(revealed2, secret2)
mse_c_S1 = MSE(stego1, carrier1)
mse_c_S2 = MSE(stego2, carrier2)
#### PSNR ### 
revealed1_f = tf.image.convert_image_dtype(revealed1.detach().numpy(), tf.float32)
revealed2_f = tf.image.convert_image_dtype(revealed2.detach().numpy(), tf.float32)
secret1_f = tf.image.convert_image_dtype(secret1.detach().numpy(), tf.float32) 
secret2_f = tf.image.convert_image_dtype(secret2.detach().numpy(), tf.float32) 
stego1_f = tf.image.convert_image_dtype(stego1.detach().numpy(), tf.float32)
stego2_f = tf.image.convert_image_dtype(stego2.detach().numpy(), tf.float32)
carrier1_f = tf.image.convert_image_dtype(carrier1.detach().numpy(), tf.float32) 
carrier2_f = tf.image.convert_image_dtype(carrier2.detach().numpy(), tf.float32) 

psnr_r_s1= tf.image.psnr(revealed1_f,secret1_f,max_val=1)
psnr_r_s2= tf.image.psnr(revealed2_f,secret2_f,max_val=1)
psnr_c_s1= tf.image.psnr(stego1_f,carrier1_f,max_val=1)
psnr_c_s2= tf.image.psnr(stego2_f,carrier2_f,max_val=1)

print(f" Carrier vs Stego :: ssim : {ssim_c_s1} , MSE : {mse_c_S1}  , PSNR : {psnr_c_s1} //// Revealed vs Secret :: ssim : {ssim_r_s1} , MSE : {mse_r_S1}  , PSNR : {psnr_r_s1}")
print(f" Carrier vs Stego :: ssim : {ssim_c_s2} , MSE : {mse_c_S2}  , PSNR : {psnr_c_s2} //// Revealed vs Secret :: ssim : {ssim_r_s2} , MSE : {mse_r_S2}  , PSNR : {psnr_r_s2}")

fig, ax = plt.subplots(1,4, figsize =(15,15))
ax[0].imshow(x[2].permute(1,2,0))
ax[1].imshow(x[7].permute(1,2,0))
ax[2].imshow(stego_x1[0].permute(1,2,0))
ax[3].imshow(revealed_x1[0].permute(1,2,0))
ax[0].set_title("Cover Image")
ax[1].set_title("Secret Image")
ax[2].set_title("Stego Image")
ax[3].set_title("revealed Image")
fig, ax = plt.subplots(1,4, figsize =(15,15))
ax[0].imshow(x[0].permute(1,2,0))
ax[1].imshow(x[1].permute(1,2,0))
ax[2].imshow(stego_x2[0].permute(1,2,0))
ax[3].imshow(revealed_x2[0].permute(1,2,0))
ax[0].set_title("Cover Image")
ax[1].set_title("Secret Image")
ax[2].set_title("Stego Image")
ax[3].set_title("revealed Image")

